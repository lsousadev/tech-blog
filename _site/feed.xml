<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://lucas-sousa.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://lucas-sousa.com/" rel="alternate" type="text/html" /><updated>2022-08-07T15:02:19-07:00</updated><id>https://lucas-sousa.com/feed.xml</id><title type="html">Tech Notebook by Lucas</title><subtitle>This blog was created for note-keeping purposes, not educational. Browse at your own peril.</subtitle><author><name>Lucas Sousa</name></author><entry><title type="html">Azure App Service Architecture</title><link href="https://lucas-sousa.com/azure-app-services-architecture/" rel="alternate" type="text/html" title="Azure App Service Architecture" /><published>2022-07-17T00:00:00-07:00</published><updated>2022-07-17T00:00:00-07:00</updated><id>https://lucas-sousa.com/azure-app-services-architecture</id><content type="html" xml:base="https://lucas-sousa.com/azure-app-services-architecture/"><![CDATA[<!--üî¥ üü† ‚ö´ ‚ö™ üü£ üü¢ üü° üîµ-->

<h2 id="app-service-within-azure-architecture">App Service Within Azure Architecture</h2>

<p><img src="/assets/images/azure_antares_architecture.png" alt="Azure - Antares Architecture" /></p>

<ul>
  <li>The architecture for App Service is called Antares.</li>
  <li>The Control Plane API requests start from ARM (Azure Resource Manager) to Antares‚Äô Geomaster.</li>
  <li>The Geomaster is a component that serves as a load balancer keeping track of state of App Service‚Äôs stamps (scale units) around the world and directing API call traffic.</li>
  <li>There are dozens to thousands of stampsin each Azure region.</li>
  <li>Each scale unit, or stamp, consists of ~1000 virtual machines, also called servers or workers.</li>
  <li>The underlying physical technology for the virtual machines are generally a single server host per VM.</li>
  <li>All servers in a stamp are used for the App Service offering only, no other Azure service.</li>
</ul>

<h2 id="app-service-creation-flow">App Service Creation Flow</h2>

<p><img src="/assets/images/app_service_creation_flow.png" alt="App Service app creation flow" /></p>

<ol>
  <li>User makes a request to create a new site.</li>
  <li>ARM makes sure user has access to the resource to allow the given operation (create in this case) and forwards the requests to App Service Geo-Master.</li>
  <li>Geo-Master finds the best suitable scale unit for the user‚Äôs request and forwards the request.</li>
  <li>The scale unit creates the new application.</li>
  <li>Geo-Master reports success on the create request.</li>
</ol>

<h2 id="stamp-architecture">Stamp Architecture</h2>

<p><img src="/assets/images/app_service_stamp_architecture.png" alt="App Service stamp architecture" /></p>

<ul>
  <li>There are different types of workers inside of a stamp:
    <ul>
      <li><strong>Web workers</strong>: the vast majority of workers in a stamp. It‚Äôs the server that runs the app. They can be shared between clients or dedicated to a single client, depending on the App Service Plan.</li>
      <li><strong>Front end</strong>: a layer 7 (HTTP) load balancer that distributes requests too all web workers allocated for an app. It uses round-robin by default.</li>
      <li><strong>File servers</strong>: worker that mounts page blob containing data needed to run app (code, images, etc) and exposes it as a network drive, which in turn is mapped by the web worker as a local drive. Any file-relateed r/w operation performed by the app passes through a file server.</li>
      <li><strong>API controller</strong>: performs management operations inside stamp. It is to the Control Plane what the Front end is to the Runtime Plane. Receives API calls and orchestrates the steps to fulfill the requests. Examples:
        <ul>
          <li>When Geo-Master passes an API call to create a new application, the API controller orchestrates the required steps to create the application at the scale unit.</li>
          <li>When you use the Azure portal to reset your application, it‚Äôs the API controller that notifies all Web Workers currently allocated to your application to restart your app.</li>
        </ul>
      </li>
      <li><strong>Azure SQL</strong>: persists app metadata, only accessed by the data role.</li>
      <li><strong>Data role</strong>: a cache layer between the SQL database and all the other workers. Examples:
        <ul>
          <li>Web workers ask Data role for website configuration.</li>
          <li>Front end workers ask Data role for list of workers they can route an app‚Äôs requests to.</li>
        </ul>
      </li>
      <li><strong>Publisher</strong>: exposes FTP functionality for customers to access their application content and logs in the Blob Storages and file servers. It also gives customers another way of deploying apps.</li>
    </ul>
  </li>
</ul>

<h2 id="iis-overview">IIS Overview</h2>

<p><img src="/assets/images/iis_request_runtime_flow.png" alt="IIS request runtime flow" /></p>

<ul>
  <li>Antares, in its initial version, was more of an IIS-as-a-service.</li>
  <li>IIS‚Äô model for hosting app code:
    <ul>
      <li>Config concepts: sites, bindings, apps, VirtualDirectories, ApplicationPools, handler mappings, etc</li>
      <li>Runtime entities: HTTP(.sys) bindings, HTTP(.sys) request queues, user identities, worker processes (w3wp.exe), etc</li>
    </ul>
  </li>
</ul>

<h2 id="web-worker-architecture">Web Worker Architecture</h2>

<p><img src="/assets/images/antares_dynamic_prov_control.png" alt="Antares dynamic website provisioning - control flow" /></p>

<ul>
  <li>Basic components:
    <ul>
      <li><strong>HTTP.sys</strong>: receives requests (by matching, as seen above) based on URL and port, then sends it to HTTP Request Queue.</li>
      <li><strong>HTTP Request Queue</strong>: sends requests received via site-specific binding (hostname + port, eg. ‚Äúmysite:80‚Äù) to the site specific HTTP request queue, and sends all other requests to the MiniARR HTTP request queue.</li>
      <li><strong>MiniARR Worker Process</strong>: only receives requests for websites not yet set up. Tells DWAS to create the structure for the website requested.</li>
      <li><strong>DWAS</strong>: receives input from two components:
        <ul>
          <li>From <strong>MiniARR</strong>, triggering a dynamic website provisining. DWAS:
            <ol>
              <li>receives host name from MiniArr and uses it to fetch site config (<code class="language-plaintext highlighter-rouge">StartSiteContext</code>) from DataRole.</li>
              <li>generates <code class="language-plaintext highlighter-rouge">applicationHost.config</code>, <code class="language-plaintext highlighter-rouge">rootweb.config</code>, etc into its temp directory.
                <ul>
                  <li>Both config files are created with ‚Äútransform pipeline‚Äù code, under <code class="language-plaintext highlighter-rouge">Microsoft.Web.Hosting\Utilities\Transformers</code>.</li>
                  <li>The transform pipeline can inject user-specific config settings into the <code class="language-plaintext highlighter-rouge">.config</code> files, such as Virtual Directories, Apps, Handler Mappings (PHP/Python support), etc.</li>
                </ul>
              </li>
              <li>generates low-privileged user identity to run site code.</li>
              <li>creates a local directory for site, and sets up a symlink to the site‚Äôs root directory. (eg. <code class="language-plaintext highlighter-rouge">C:\DWASFiles\Sites\foo\VirtualDir0</code> pointing to <code class="language-plaintext highlighter-rouge">\\FileServerIP1\volume-3-default\&lt;webspaceGuid&gt;\&lt;siteGuid&gt;</code>)</li>
              <li>copies config files from temp to local directory at <code class="language-plaintext highlighter-rouge">config\</code>.</li>
              <li>creates a site-specific binding in HTTP.sys and an HTTP request queue, asking to be notified once queue gets a requests.</li>
              <li>registers with Data Role to receive change notifications to website config.</li>
              <li>sets up rest of state in machine needed to run the site (VNet and MSI integration, cert installation, local cache hydration, etc), based on site config.</li>
            </ol>
          </li>
          <li>From <strong>site-specific HTTP request queue</strong>, triggering DWAS to:
            <ol>
              <li>spin up and initializes a sandboxed Worker Process (a <code class="language-plaintext highlighter-rouge">w3wp.exe</code>) for the site.</li>
              <li>create a sandbox by virtualizing <code class="language-plaintext highlighter-rouge">D:\home</code> which points to the site‚Äôs root directory and only allows access to that SMB path.
                <ul>
                  <li>SANDBOX</li>
                </ul>
              </li>
            </ol>
          </li>
          <li>From <strong>DataRole</strong> (outside of Web Worker) to change site configuration. DWAS:
            <ol>
              <li>receives a notification from DataRole by long-polling for it.</li>
              <li>fetches new <code class="language-plaintext highlighter-rouge">StartSiteContext</code> from DataRole and compares with previous version.</li>
              <li>overwrites <code class="language-plaintext highlighter-rouge">.config</code> files if there are changes and orchestrates the changes to ensure new config is applied.
                <ul>
                  <li>Overwriting the <code class="language-plaintext highlighter-rouge">.config</code> files causes AppDomain recycles for ASP.NET apps, and can cause child-process recycles for other stacks.</li>
                  <li>Config changes such as an update to app-settings usually require worker process recycle.</li>
                </ul>
              </li>
              <li>gets informed about internal system changes such as movement of storage volumes through long-polling calls to DataRole as well, on top of pinging storage itself every 5 seconds for health.
                <ul>
                  <li>Change to the site‚Äôs root path requires worker process recycle.</li>
                </ul>
              </li>
            </ol>
          </li>
        </ul>
      </li>
      <li><strong>Worker Process</strong>: sandboxed site-specific worker process that dequeues requests in its corresponding queue, then processes it through a module pipeline, much like in IIS (out of scope for this post).
        <ul>
          <li>Its initialization process is similar to IIS as well, requiring DWAS to guide it through start-up, and then signal when it is ready to start processing requests.</li>
          <li>The process is under a ‚Äújob object‚Äù, which imposes restrictions on the amount of memory and CPU that specific <code class="language-plaintext highlighter-rouge">w3wp.exe</code> can utilize.</li>
          <li>Again, like IIS, the worker process expects to retrieve config files from the DWAS local directory at <code class="language-plaintext highlighter-rouge">C:\DWASFiles\Sites\foo\config\</code>.</li>
          <li>The worker process stays alive for 30 minutes before ending itself if there are no requests.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="reference">Reference</h2>

<ul>
  <li><a href="https://docs.microsoft.com/en-us/archive/msdn-magazine/2017/february/azure-inside-the-azure-app-service-architecture">https://docs.microsoft.com/en-us/archive/msdn-magazine/2017/february/azure-inside-the-azure-app-service-architecture</a></li>
</ul>]]></content><author><name>Lucas</name></author><category term="azure" /><category term="azure" /><category term="cloud" /><category term="vnet" /><category term="app service" /><category term="dns" /><category term="networking" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Azure Networking for Virtual Networks</title><link href="https://lucas-sousa.com/azure-networking-vnet-dns/" rel="alternate" type="text/html" title="Azure Networking for Virtual Networks" /><published>2022-07-10T00:00:00-07:00</published><updated>2022-07-10T00:00:00-07:00</updated><id>https://lucas-sousa.com/azure-networking-vnet-dns</id><content type="html" xml:base="https://lucas-sousa.com/azure-networking-vnet-dns/"><![CDATA[<!--üî¥ üü† ‚ö´ ‚ö™ üü£ üü¢ üü° üîµ-->

<h2 id="basics--concepts">Basics &amp; Concepts</h2>

<h3 id="vnet--dns">VNet &amp; DNS</h3>

<ul>
  <li>The VNet uses Azure DNS by default.
    <ul>
      <li>NIC‚Äôs inside a VNet use the VNet DNS configuration by default.</li>
    </ul>
  </li>
  <li>An internal DNS private zone called IDNS is created automatically inside every VNet.
    <ul>
      <li>Any NIC added to the VNet gets registered within IDNS</li>
      <li>The domain name for the private zone is <code class="language-plaintext highlighter-rouge">internal.cloudapp.net</code></li>
    </ul>
  </li>
</ul>

<h3 id="private-dns-zone">Private DNS Zone</h3>

<ul>
  <li>Exists outside of VNets (global resource) and can be attached to them.</li>
  <li>Can hold any time of record for any name. Full-range.</li>
  <li>Private DNS zones can be used solely for DNS resolution or for registration:
    <ul>
      <li>Like IDNS, all NIC‚Äôs added to the VNet gets registered within the private zone.</li>
      <li>A VNet can only connect to one private zone for registration.</li>
      <li>A private DNS zone can connect to 100 VNets for registration and 1000 for resolution.</li>
    </ul>
  </li>
</ul>

<h3 id="network-security-groups">Network Security Groups</h3>

<ul>
  <li>Can only be attached to a subnet or a network interface (eg. a VM).</li>
  <li>App Security Group: works like a managed tag that can be referenced as source/destination in NSG rules.</li>
</ul>

<h3 id="the-azure-backbone">The Azure ‚ÄúBackbone‚Äù</h3>

<ul>
  <li>Microsoft has a network infrastructure for Azure called the ‚ÄúAzure backbone‚Äù.</li>
  <li>Azure can ‚Äúmagically‚Äù route things through this backbone.</li>
  <li>This backbone is how service endpoint, private endpoint, and VNet integration can keep requests needing to reach the internet.
    <ul>
      <li>Service endpoints use that magic by forwarding all requests to public IPs of storage accounts to a <code class="language-plaintext highlighter-rouge">VirtualNetworkServiceEndpoint</code>, which then redirects the requests through the backbone and is fully managed by Azure, the customer can‚Äôt see it.</li>
    </ul>
  </li>
</ul>

<h3 id="service-endpoints">Service Endpoints</h3>

<p><img src="/assets/images/service_endpoint.jpg" alt="Service Endpoint Request Flow" /></p>

<ul>
  <li>Adds a record to a subnet‚Äôs route table pointing public IP addresses of services to <code class="language-plaintext highlighter-rouge">VirtualNetworkServiceEndpoint</code>, which chains records redirecting the request through the Azure backbone all the way to the service‚Äôs public endpoint.</li>
  <li>The source IP address will switch from public to private as the request will be internal to the VNet, so the service‚Äôs firewall rules must reflect that.</li>
  <li>The request‚Äôs final hop is still the public endpoint of the target <em>service</em>, even if the source IP is private.</li>
  <li>Services such as Azure Storage, Key Vault, Cosmos DB, etc offer service endpoint.</li>
</ul>

<h3 id="private-endpoints">Private Endpoints</h3>

<p><img src="/assets/images/private_endpoint_private_link.png" alt="Private Endpoint Request Flow with Private Link" /></p>

<ul>
  <li>Adds a NIC to a subnet and attaches said NIC to a specific <em>resource</em> (eg. a blob storage).</li>
  <li>It is usually coupled with Private Link, which creates a private DNS zone to map all resource requests to their correct Private Endpoint private IP addresses.</li>
</ul>

<h2 id="example">Example</h2>

<h3 id="nslookup-testblobcorewindowsnet-from-a-vm-in-azure"><code class="language-plaintext highlighter-rouge">nslookup test.blob.core.windows.net</code> from a VM in Azure</h3>

<ul>
  <li>without any integrations:
    <ul>
      <li>‚ö™ test.blob.core.windows.net -&gt; CNAME -&gt; üîµ blob.a1b2c3.store.core.windows.net</li>
      <li>üîµ blob.a1b2c3.store.core.windows.net -&gt; A record -&gt; üî¥ 52.240.48.36 (public IP)</li>
    </ul>
  </li>
  <li>with Service Endpoint:
    <ul>
      <li>‚ö™ test.blob.core.windows.net -&gt; CNAME -&gt; üîµ blob.a1b2c3.store.core.windows.net</li>
      <li>üîµ blob.a1b2c3.store.core.windows.net -&gt; A record -&gt; üî¥ 52.240.48.36 (public IP)</li>
      <li>üî¥ 52.240.48.36 (public IP) -&gt; Subnet Route Table Entry -&gt; üü† VirtualNetworkServiceEndpoint</li>
      <li>üü† VirtualNetworkServiceEndpoint -&gt; ???? -&gt; ‚ö´ Azure Backbone</li>
      <li>‚ö´ Azure Backbone -&gt; ???? -&gt; üî¥ 52.240.48.36 (????)</li>
    </ul>
  </li>
  <li>with Private Link:
    <ul>
      <li>‚ö™ test.blob.core.windows.net -&gt; CNAME -&gt; üü° test.privatelink.blob.core.windows.net</li>
      <li>üü° test.privatelink.blob.core.windows.net -&gt; CNAME -&gt; üîµ blob.a1b2c3.store.core.windows.net</li>
      <li>üîµ blob.a1b2c3.store.core.windows.net -&gt; A record -&gt; üî¥ 52.240.48.36 (public IP)</li>
    </ul>
  </li>
  <li>with Private Link and Private Endpoint:
    <ul>
      <li>‚ö™ test.blob.core.windows.net -&gt; CNAME -&gt; üü° test.privatelink.blob.core.windows.net</li>
      <li>üü° test.privatelink.blob.core.windows.net -&gt; A record (Azure Private DNS) -&gt; üü¢ 10.0.1.4</li>
    </ul>
  </li>
</ul>

<h3 id="the-reason-for-privatelink">The reason for PrivateLink</h3>

<p>From the results above, it may seem that if you have a Private Endpoint, you can bypass PrivateLink and just create an Azure Private DNS that points ‚ö™ test.blob.core.windows.net to üü¢ 10.0.1.4. It would work, but for that to happen the Private DNS zone would have to be at the <code class="language-plaintext highlighter-rouge">blob.core.windows.net</code>, and you‚Äôd then be expected to create a record for each Blob Storage that the VNet attached to the Private DNS zone would like to connect to. For example, if a machine also wanted to connect to <code class="language-plaintext highlighter-rouge">second-test.blob.core.windows.net</code>, it would be directed to resolve with the Private DNS zone, which would need to be manually configured. It is often more sensible to create a Private DNS zone at the <code class="language-plaintext highlighter-rouge">privatelink.blob.core.windows.net</code> level.</p>

<h3 id="on-premises-considerations">On-premises considerations</h3>

<ul>
  <li>Azure Private DNS zones are available globally, you can connect any VNet in the world to it for link resolution, but you cannot connect to it from on-premises.</li>
  <li>You can deploy a DNS forwarder in a VNet, then create a record in the local DNS (on-prem) to point any <code class="language-plaintext highlighter-rouge">privatelink.blob.core.windows.net</code> requests to the DNS forwarder, which can then talk to the Private DNS zone.</li>
  <li>Optionally, the records in the Private DNS zone can be recreated manually on local DNS.</li>
</ul>

<h3 id="privatelink-service">PrivateLink Service</h3>

<ul>
  <li>Useful in cases virtual networks cannot be peered (eg. 3rd party services, CIDR block overlap, etc).</li>
  <li>The host VNet deploys PLS internally in its own subnet. PLS will receive external requests and forward them to a Standard Load Balancer.</li>
  <li>The PLS deployment can have up to 8 IP addresses in the VNet, and it connects to a Front End IP address of the Load Balancer.</li>
</ul>

<h2 id="questions">Questions</h2>

<ul>
  <li>Is the VirtualNetworkServiceEndpoint a NIC with private IP? Is there a DNS record for VirtualNetworkServiceEndpoint? What does it look like?</li>
  <li>What does a traceroute of a request to the public IP of a service looks like compared a request to VirtualNetworkServiceEndpoint?</li>
  <li>Does the Private Endpoint NIC get attached directly to the resource server?</li>
  <li>How does Private Endpoint DNS work without Private Link?</li>
  <li>How does a private DNS zone get linked to a VNet? VNet‚Äôs route table?</li>
</ul>]]></content><author><name>Lucas</name></author><category term="networking" /><category term="azure" /><category term="azure" /><category term="cloud" /><category term="vnet" /><category term="app service" /><category term="dns" /><category term="private endpoint" /><category term="service endpoint" /><category term="private link" /><category term="networking" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">AWS re:Invent 2019: Data-Driven, Cloud-Native Ecosystem</title><link href="https://lucas-sousa.com/reinvent-2019-data-driven-cloud-native-ecosystem/" rel="alternate" type="text/html" title="AWS re:Invent 2019: Data-Driven, Cloud-Native Ecosystem" /><published>2022-01-08T00:00:00-08:00</published><updated>2022-01-08T00:00:00-08:00</updated><id>https://lucas-sousa.com/reinvent-2019-data-driven-cloud-native-ecosystem</id><content type="html" xml:base="https://lucas-sousa.com/reinvent-2019-data-driven-cloud-native-ecosystem/"><![CDATA[<!--üî¥ üü† ‚ö´ ‚ö™ üü£ üü¢ üü° üîµ-->

<p><a href="https://youtu.be/9fH3y5p8ewE">‚ÄúPresentation Video‚Äù</a></p>

<h2 id="characteristics-of-automotive-ecosystem">Characteristics of Automotive Ecosystem</h2>

<ul>
  <li>Connectivity: on-demand services</li>
  <li>Autonomous Driving: push towards full autonomy. Lots of sensors, camera readers, up to 25TB/day of data</li>
  <li>Multimodal Mobility: point A to point B</li>
  <li>Marketplace: whole list of contextual, local, location-based services like food ordering &amp; payment, hotel reservation, offer from dealership</li>
  <li>Electrification: very pervasive on ride-sharing and micro mobility (scooters, bikes) solutions</li>
  <li>Subscriptions: car makers are becoming software and service companies (eg: make cars for subscription)</li>
</ul>

<p><img src="https://i.imgur.com/vcQA78X.png" alt="&quot;Why Enterprises Need Data Lakes&quot;" /></p>

<h2 id="three-main-features-of-data-lakes">Three main features of Data Lakes</h2>

<ol>
  <li>Single point of truth: work on same data without moving it around, from R&amp;D to manufacturing to logistics to after-sales. Democratizing the data usage in the enterprise.</li>
  <li>Support various data formats: from rational to non-rational data. Car sensor data, images from autonomous driving, etc.</li>
  <li>Scales storage and compute individually.</li>
</ol>

<h2 id="intro-to-bmws-solution">Intro to BMW‚Äôs Solution</h2>

<ul>
  <li>BMW Intelligent Personal Assistant (IBM Assistant) - ‚ÄúSoul of your BMW‚Äù - focused on becoming more connected and more contextualized</li>
  <li>Has to be properly integrated with Manufacturing, Logistics, Customer Service, After-Sales</li>
  <li>They want to transform BMW into a truly data-driven company</li>
  <li>Driven by:
    <ul>
      <li>Data &amp; AI</li>
      <li>Cloud stack: pooling and providing the right tools and frameworks to work with data in most effective ways - leverage good, complete cloud stack - move away from heterogeneous landscape</li>
      <li>Emerging tech: novel/unproven tech (eg: natural language processing, blockchain, etc) shouldn‚Äôt be avoided</li>
    </ul>
  </li>
</ul>

<h2 id="motivation">Motivation</h2>

<p>(Why BMW started aggressively pursuing cloud solutions in 2019)</p>

<ul>
  <li>2014: Big Data journey - Hadoop + Kafka stack (fueling data transformation on top of the Hadoop ecosystem). Ingested both IoT and Product (ERP, CRM, OLTP) data via Kafka or Spark.</li>
  <li>Eventually purpose-built databases started being introduced to speed up workloads (eg: PostgreSQL) trying to satisfy customer requirements</li>
  <li>Lastly, to harness the results of the data analysis, in 2016 a K8s cluster was introduced.</li>
</ul>

<p><img src="https://i.imgur.com/CPuqyUF.png" alt="&quot;Old Big Data Landscape&quot;" /></p>

<p>Very heterogeneous landscape. Consequences:</p>

<ul>
  <li>Only fractions of critical organization‚Äôs data were able to be ingested.</li>
  <li>Bottlenecks were prominent. There were central teams taking care of data ingestion, no self-service. Scaling workforce didn‚Äôt help.</li>
  <li>Move to purpose-built sandboxes for experiments (eg: Docker, Jupiter Notebooks, traditional solutions, etc).</li>
  <li>The number of prototypes spinning out in different environments was growing exponentially bigger than products being made.</li>
</ul>

<p>The whole (small) DevOps team had full and trained responsibilities over all the components. Those reasons led BMW to move to the cloud.</p>

<p><img src="https://i.imgur.com/YdGDQe1.png" alt="&quot;Lift, Think, and Shift&quot;" /></p>

<p>Lift, Think, &amp; Shift: re-architecture of major parts of the on-premises platform into the cloud, not just duplicate the combobulated environment it currently had. Move of major parts into AWS primarily built upon cloud-native building blocks and IaaC</p>

<p>That shift allowed BMW to focus more on the differentiating ‚Äúbits and pieces‚Äù (services) and the quality of their data rather than making sure the tools, platforms, and infrastructures worked and communicated properly</p>

<p>Hypothesis that supported the move to the cloud:</p>

<ul>
  <li>BMW will not be able to keep up with the radical pace of innovation AWS has in their data lakes services portfolio</li>
  <li>making sure that BMW understands, curates, transforms, and leverages their data as opposed to wasting time with anything else</li>
</ul>

<h2 id="bmw-groups-cloud-data-hub">BMW Group‚Äôs Cloud Data Hub</h2>

<p>(Platform design of the Cloud Data Hub built upon AWS)</p>

<p>Three main premises:</p>

<ol>
  <li>Enable a very quick infrastructure setup in different regions while maintaining high security standards and complying with regulations</li>
  <li>Support quick data integration for a wide array of datasets (both streaming and relational)</li>
  <li>Provide consistent means of how people will interact with the data and how identities are managed across the platform (eg: SSO, making sure it translates well into the enterprise scenarios)</li>
</ol>

<p>Three-layered approach:</p>

<p><img src="https://i.imgur.com/1RoHI0U.png" alt="&quot;Three Layered Approach&quot;" /></p>

<ol>
  <li>Real-Time Zone: keeps cached copies of real-world objects (eg: customer vehicles) for fast access to non-historic sized data needed for inference of ML models</li>
  <li>Insight Zone: historicized versions of real-time objects and also plays well with already existing relational databases (OLTP, data warehouses, etc)</li>
  <li>Unified API: collection of restful API that makes interaction of people &amp; systems w/ data easy, and gives the ability to expose both real-time and relational data very effectively</li>
</ol>

<p>Three important things about data platform:</p>

<p><img src="https://i.imgur.com/jzaPUlE.png" alt="&quot;Three Important Data Platform Facts&quot;" /></p>

<ul>
  <li>Democratization: provide easy access to both bounded and unbounded data. Single point of truth for any kind of data analytics. Enable data stewards to intuitively look at what‚Äôs already there. Introducing a data catalog built in-house upon the existing Cloud Data Hub for exploration</li>
  <li>Modularity: key to enable broader community to work with the platform. Allows users to bring their own building blocks too</li>
  <li>Lean Ops: avoid putting all the effort and energy into the system, let AWS handle infrastructure. Fast-paced environment to drive innovation</li>
</ul>

<h2 id="cloud-data-hub-gui">Cloud Data Hub GUI</h2>

<p><img src="https://i.imgur.com/tqToANn.png" alt="&quot;BMW Clodu Data Hub GUI&quot;" /></p>

<p>Client Flow:</p>

<ol>
  <li>Login</li>
  <li>Select Role</li>
  <li>Launch CDH</li>
  <li>Make queries</li>
</ol>

<p>Providing Data: very difficult, many ETL tools but always hits limitations and bottlenecks when implementing.</p>

<ul>
  <li>Option 1: Terraform template</li>
  <li>Option 2: Manually via Hub (config &amp; auth, ingest source, scheduling)</li>
</ul>

<p>Older flow used EMR. Newer flow uses Lambda, CDK, CloudFormation orchestration.</p>

<h3 id="data-consumption">Data Consumption</h3>

<ul>
  <li>Consumer requirements for data consumption: SPARK. Many data analytics points start with Spark.</li>
  <li>Cloud Data Hub gives customers the ability to spin up a customizable Sage Maker stack with an EMR cluster. You can also open a Jupyter Notebook that directly interacts with the EMR cluster.</li>
</ul>

<h3 id="data-ingestion">Data Ingestion</h3>

<p><img src="https://i.imgur.com/SMmdzOQ.png" alt="&quot;Data Ingestion - Part 1&quot;" /></p>

<p><img src="https://i.imgur.com/1cAvlN3.png" alt="&quot;Data Ingestion - Part 2&quot;" /></p>

<p>DATA.BMW.CLOUD: Store all bounded and unbounded data, build tools on top of that (eg: BMW CDH Portal) to give great and easy customer experience.</p>

<p>Data Provider: very easy and customizable to bring data into CDH:</p>

<p><img src="https://i.imgur.com/wqNihtj.png" alt="&quot;Data Provider Code&quot;" /></p>

<h3 id="whats-provisioned-by-the-terraform-module-inside-the-data-provider">What‚Äôs provisioned by the Terraform module inside the Data Provider</h3>

<ul>
  <li>
    <p>Bounded data:</p>

    <p><img src="https://i.imgur.com/AeDluFz.png" alt="&quot;Ingestion Module - Bounded Data&quot;" /></p>

    <ul>
      <li><code class="language-plaintext highlighter-rouge">Glue Trigger</code>: triggers the Glue ETL</li>
      <li><code class="language-plaintext highlighter-rouge">Glue ETL</code>: has access to on-prem DB since it‚Äôs inside Private VPC. Uses:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">CloudWatch</code>: log metrics</li>
          <li><code class="language-plaintext highlighter-rouge">Secrets Manager</code>: DB secrets</li>
          <li><code class="language-plaintext highlighter-rouge">Local Glue Data Catalog</code>: metadata</li>
        </ul>
      </li>
      <li><code class="language-plaintext highlighter-rouge">Source S3 bucket</code>: stores all data. Resources for new data created via API</li>
      <li><code class="language-plaintext highlighter-rouge">Glue DC Sync &amp; Global Glue DC</code>: all metadata stored and made available to consumer accounts</li>
    </ul>
  </li>
  <li>
    <p>Unbounded Data:</p>
  </li>
</ul>

<p><img src="https://i.imgur.com/kMFa4OL.png" alt="&quot;Ingestion Module - Ingredients&quot;" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">Kinesis</code>: Receives data from cloud-native data providers
    <ul>
      <li>1-2 minutes latency from writing data to Kinesis until it is visible in Athena</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Firehose</code>: Batch data to S3</li>
  <li><code class="language-plaintext highlighter-rouge">Lambda</code>: transforms data</li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">Fargate</code>: Imports legacy Kafka topics into Kinesis</p>

    <p><img src="https://i.imgur.com/q9wmF9t.png" alt="&quot;Ingestion Module - Unbouded Data&quot;" /></p>
  </li>
  <li>Lambda Prep &amp; S3 Prep Bucket: Event data enriched with data such as vehicle/customer data.</li>
  <li>Bucket Policies in the Source Bucket only apply to the objects owned by the bucket owner. That makes it hard for the Source Bucket to give access to consumer accounts. The solution is to use STS and roles when allowing Data Providers to feed data to the bucket.</li>
  <li>For services that aren‚Äôt ‚Äúrole-aware‚Äù like Firehose, the workaround is to trigger an event on the upload, which uses SNS and SQS to start a Lambda function that fixes ownership of the object in the bucket and before passing the data along the pipeline to be enriched.</li>
</ul>

<h2 id="cloud-data-hub-highlights">Cloud Data Hub highlights</h2>

<ol>
  <li>Faster and more real-time: increase latency (eg: introduce Kinesis Data Analytics)</li>
  <li>Metadata and lineage: improve metadata handling and data lineage tracking (from source to semantic layer), not currently reachable through API</li>
  <li>Search in (meta-)data: improve indexing</li>
  <li>Building blocks: improve customer service by building more blocks for data providers (eg: elasticsearch integration)</li>
  <li>Increased self-service: more features to service portal (eg: make Terraform modules available on UI)</li>
</ol>

<h2 id="data-consumers">Data Consumers</h2>

<ul>
  <li>Where data scientists and analysts use consumer accounts and advanced analytics services such as Sage Maker. It‚Äôs important that consumers have the freedom to design their own custom infrastructure along with the available building blocks and templates.</li>
  <li>Consumer account can also use SNS topic to trigger the processing once new data arrives.</li>
</ul>]]></content><author><name>Lucas</name></author><category term="aws" /><category term="aws" /><category term="re:invent" /><category term="cloud" /><category term="automotive" /><category term="bmw" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Kubernetes Basics</title><link href="https://lucas-sousa.com/kubernetes-basics/" rel="alternate" type="text/html" title="Kubernetes Basics" /><published>2021-12-18T00:00:00-08:00</published><updated>2021-12-18T00:00:00-08:00</updated><id>https://lucas-sousa.com/kubernetes-basics</id><content type="html" xml:base="https://lucas-sousa.com/kubernetes-basics/"><![CDATA[<!--üî¥ üü† ‚ö´ ‚ö™ üü£ üü¢ üü° üîµ-->

<h2 id="references">References</h2>

<p><a href="https://www.freecodecamp.org/news/the-kubernetes-handbook/#introduction-to-container-orchestration-and-kubernetes">The Kubernetes Handbook</a></p>

<p><a href="https://github.com/fhsinchy/kubernetes-handbook-projects">GitHub - fhsinchy/kubernetes-handbook-projects</a></p>

<h2 id="useful-code-snippets">Useful Code Snippets</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create 'pod/&lt;pod_name&gt;'</span>
<span class="c"># Run &lt;image_name&gt; inside pod</span>
kubectl run &lt;pod_name&gt; <span class="se">\</span>
<span class="nt">--image</span><span class="o">=</span>&lt;image_name&gt; <span class="se">\</span>
<span class="nt">--port</span><span class="o">=</span>80
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 'ls -la' for k8s</span>
kubectl get pod/service
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create 'service/&lt;pod_name&gt;'</span>
<span class="c"># Run load balancer as a service</span>
kubectl expose pod &lt;pod_name&gt; <span class="se">\</span>
<span class="nt">--type</span><span class="o">=</span>LoadBalancer <span class="se">\</span>
<span class="nt">--port</span><span class="o">=</span>80
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Delete resource based on type and name</span>
kubectl delete &lt;<span class="nb">type</span><span class="o">&gt;</span> &lt;name&gt;
</code></pre></div></div>

<h2 id="basics">Basics</h2>

<p>üö¢ Container orchestration is the process of automating the <code class="language-plaintext highlighter-rouge">deployment</code>, <code class="language-plaintext highlighter-rouge">management</code>, <code class="language-plaintext highlighter-rouge">scaling</code>, and <code class="language-plaintext highlighter-rouge">networking</code> tasks of containers.</p>

<h3 id="architecture">Architecture</h3>

<p><img src="/assets/images/2021-12-18-kubernetes-basics/k8s_architecture.png" alt="Kubernetes Architecture" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">kubectl</code>: K8s CLI tool to communicate with your cluster via API</li>
  <li><code class="language-plaintext highlighter-rouge">node</code>: the machine, virtual or physical, that hosts the pods. Usually refers to worker nodes that run the containerized workloads, but could also refer to control plane nodes (centralized API, cluster management).
    <ul>
      <li><code class="language-plaintext highlighter-rouge">kubelet</code>: primary agent in a node, does things such as start pods &amp; containers or register the node with the apiserver. Receives instructions from the control plane, keeps etcd updated</li>
      <li><code class="language-plaintext highlighter-rouge">kube-proxy</code>: runs on each node to maintain network rules. Any network request that reaches a service inside the cluster passes through the kube-proxy service</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">control plane</code>: cluster management
    <ul>
      <li><code class="language-plaintext highlighter-rouge">etcd</code>: data store that maintains a record of the state of the cluster in a distributed key-value store</li>
      <li><code class="language-plaintext highlighter-rouge">kube-scheduler</code>: queries state of the cluster (at etcd) to schedule/assign workloads to worker nodes, making sure no server is overloaded</li>
      <li><code class="language-plaintext highlighter-rouge">kube-api-server</code>: gateway to query and interact with cluster. Validates and processes requests delivered using client libraries like kubectl</li>
      <li><code class="language-plaintext highlighter-rouge">cloud-controller-manager</code>: interacts with cloud provider‚Äôs (GKE, EKS) API. Keeps local and cloud interactions isolated</li>
      <li><code class="language-plaintext highlighter-rouge">kube-controller-manager</code>: responsible for controlling the state of the cluster. Implements and tracks the lifecycle of the controllers deployed to the cluster. Main workloads managed by kube-controller-manager:
        <ul>
          <li><code class="language-plaintext highlighter-rouge">Deployment</code>: declarative and good for managing stateless apps (pods are interchangeable and replaceable), it matches the current state of your cluster to the desired state mentioned in the Deployment manifest (YAML). e.g. If you create a deployment with 1 replica, it will check that the desired state of ReplicaSet is 1 and current state is 0, so it will create a ReplicaSet, which will further create the pod.</li>
          <li><code class="language-plaintext highlighter-rouge">StatefulSet</code>: lets you run 1+ pods that track state in some manner. Helps match pods with persistent volumes.</li>
          <li><code class="language-plaintext highlighter-rouge">DaemonSet</code>: ensures that the pod specified via YAML runs on all the nodes of the cluster. If a node is added/removed from a cluster, DaemonSet automatically adds/deletes the pod. e.g. Installing a monitoring agent on each node.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="objects">Objects</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">pods</code>: self-contained, easily replicable objects. Can host a container or a group of (very closely related ‚Äî more than back-end app + db) containers with shared network IP address and filesystem volumes, meant to run a single instance of an application.</li>
  <li><code class="language-plaintext highlighter-rouge">services</code>: since pods are ephemeral (destroyed and replaced, not recycled ‚Äî new IP every time), a service is an abstraction that groups a logical set of pods and presents them as a single entity, e.g. LoadBalancer, ClusterIP, NodePort, ExternalName
    <ul>
      <li><code class="language-plaintext highlighter-rouge">LoadBalancer</code>: exposes a group of pods as one entity to the outside (of the cluster). The pod(s) become(s) exposed on the port specified. Can be accessed via <code class="language-plaintext highlighter-rouge">minikube service &lt;service_name&gt;</code>, which accesses the cluster‚Äôs IP + the ephemeral exposed port, which is mapped to the port exposed by the load balancer.</li>
      <li><code class="language-plaintext highlighter-rouge">ClusterIP</code>: same as LoadBalancer but only within the cluster</li>
    </ul>
  </li>
  <li><code class="language-plaintext highlighter-rouge">Controllers</code>: non-terminating control loops that watch the state of the cluster and make/request changes accordingly. There are controllers to manage each type of workload supported by Kubernetes.</li>
</ul>

<h2 id="declarative-app-example">Declarative App Example</h2>

<h3 id="directory-tree">Directory Tree</h3>

<ul>
  <li>src /
    <ul>
      <li>(app code)</li>
    </ul>
  </li>
  <li>k8s /
    <ul>
      <li>
        <p>example-pod.yaml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span> <span class="c1"># k8s API version</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Pod</span> <span class="c1"># object type</span>
<span class="na">metadata</span><span class="pi">:</span> <span class="c1"># useful for future referencing</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">example-pod</span>
    <span class="na">labels</span><span class="pi">:</span> <span class="c1"># services select objects via labels</span>
    <span class="na">component</span><span class="pi">:</span> <span class="s">web</span>
<span class="na">spec</span><span class="pi">:</span> <span class="c1"># specify state wanted for object</span>
    <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">example</span>
    <span class="na">image</span><span class="pi">:</span> <span class="s">example/latest</span>
    <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>example-lb-service.yaml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">example-lb-service</span>
<span class="na">spec</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span> <span class="c1"># other options: ClusterIP, NodePort, ExternalName</span>
    <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">80</span> <span class="c1"># port for others to access LB pod</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="m">80</span> <span class="c1"># containers' port (must match)</span>
    <span class="na">selector</span><span class="pi">:</span> <span class="c1"># ID objects connected to the service</span>
    <span class="na">component</span><span class="pi">:</span> <span class="s">web</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>
    <p>Dockerfile</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># copies the whole directory into node image directory</span>
<span class="c"># installs package.json with npm, then runs build</span>
<span class="c"># copies compiled app into nginx image with port 80 exposed</span>
<span class="c"># after that, image can run as container with port 80 publicly accessible</span>
</code></pre></div>    </div>
  </li>
  <li>index.html</li>
  <li>package.json</li>
</ul>

<h3 id="execution">Execution</h3>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># Both 'apply' commands can be consolidated with 'kubectl apply -f &lt;directory&gt;'</span>

  kubectl apply <span class="nt">-f</span> example-pod.yaml
  
  <span class="c"># pod/example-pod created</span>
  
  kubectl get pod
  
  <span class="c"># NAME         READY   STATUS    RESTARTS   AGE</span>
  <span class="c"># example-pod   1/1     Running   0          3m3s</span>
  
  kubectl apply <span class="nt">-f</span> example-lb-service.yaml
  
  <span class="c"># service/example-lb-service created</span>
  
  kubectl get service
  
  <span class="c"># NAME                 TYPE           CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE</span>
  <span class="c"># example-lb-service   LoadBalancer   10.107.231.120   &lt;pending&gt;     80:30848/TCP   7s</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># Example if running locally with minikube</span>

  minikube service example-lb-service
  
  <span class="c"># |-----------|--------------------|-------------|-----------------------------|</span>
  <span class="c"># | NAMESPACE |        NAME        | TARGET PORT |             URL             |</span>
  <span class="c"># |-----------|--------------------|-------------|-----------------------------|</span>
  <span class="c"># | default   | example-lb-service |          80 | http://192.168.99.101:30848 |</span>
  <span class="c"># |-----------|--------------------|-------------|-----------------------------|</span>
  <span class="c"># ?  Opening service default/example-lb-service in default browser...</span>
</code></pre></div></div>

<h2 id="multi-container-app-example">Multi-Container App Example</h2>

<h3 id="architecture-1">Architecture</h3>

<p><img src="/assets/images/2021-12-18-kubernetes-basics/multi-container-app-arch.png" alt="Untitled" /></p>

<ul>
  <li>Note that the ClusterIP works like a LoadBalancer and can point to a single pod or a set of pods, but it can only serve requests from within the cluster</li>
</ul>

<h3 id="directory-tree-1">Directory Tree</h3>

<ul>
  <li>app /
    <ul>
      <li>(app source code, including Dockerfile)</li>
    </ul>
  </li>
  <li>postgres /
    <ul>
      <li>(db Dockerfiles and init file)</li>
    </ul>
  </li>
  <li>k8s /
    <ul>
      <li>
        <p>app-deployment.yaml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span> <span class="c1"># based on docs</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">app-deployment</span>
<span class="na">spec</span><span class="pi">:</span> <span class="c1"># controller specs</span>
    <span class="na">replicas</span><span class="pi">:</span> <span class="m">3</span>
    <span class="na">selector</span><span class="pi">:</span> 
    <span class="na">matchLabels</span><span class="pi">:</span>
    <span class="na">component</span><span class="pi">:</span> <span class="s">app</span> <span class="c1"># all pods with this label will be controlled</span>
    <span class="na">template</span><span class="pi">:</span> <span class="c1"># container specs and metadata</span>
    <span class="na">metadata</span><span class="pi">:</span>
    <span class="na">labels</span><span class="pi">:</span>
    <span class="na">component</span><span class="pi">:</span> <span class="s">app</span> <span class="c1"># all pods created will have this label</span>
    <span class="na">spec</span><span class="pi">:</span>
    <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">app</span>
        <span class="s">image</span><span class="err">:</span> <span class="s">app/latest</span>
        <span class="s">ports</span><span class="err">:</span>
        <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">3000</span>
        <span class="na">env</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">DB_CONNECTION</span>
        <span class="na">value</span><span class="pi">:</span> <span class="s">pg</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">DB_HOST</span> <span class="c1"># use service name to reach db host</span>
                <span class="na">value</span><span class="pi">:</span> <span class="s">postgres-cluster-ip-service</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">DB_PORT</span>
                <span class="s">value</span><span class="err">:</span> <span class="s1">'</span><span class="s">5432'</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">DB_USER</span>
                <span class="s">value</span><span class="err">:</span> <span class="s">postgres</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">DB_DATABASE</span> <span class="c1"># db to be used inside host</span>
                <span class="na">value</span><span class="pi">:</span> <span class="s">notesdb</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">DB_PASSWORD</span>
                <span class="s">value</span><span class="err">:</span> <span class="s">63eaQB9wtLqmNBpg</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>postgres-deployment.yaml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">apps/v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Deployment</span>
<span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">postgres-deployment</span>
<span class="na">spec</span><span class="pi">:</span>
    <span class="na">replicas</span><span class="pi">:</span> <span class="m">1</span>
    <span class="na">selector</span><span class="pi">:</span>
    <span class="na">matchLabels</span><span class="pi">:</span>
        <span class="na">component</span><span class="pi">:</span> <span class="s">postgres</span>
    <span class="na">template</span><span class="pi">:</span>
    <span class="na">metadata</span><span class="pi">:</span>
        <span class="na">labels</span><span class="pi">:</span>
        <span class="na">component</span><span class="pi">:</span> <span class="s">postgres</span>
    <span class="na">spec</span><span class="pi">:</span>
    <span class="na">volumes</span><span class="pi">:</span> <span class="c1"># pods' storage assignments and claims</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">postgres-storage</span>
            <span class="s">persistentVolumeClaim</span><span class="err">:</span>
            <span class="na">claimName</span><span class="pi">:</span> <span class="s">database-persistent-volume-claim</span>
        <span class="na">containers</span><span class="pi">:</span>
        <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">postgres</span>
            <span class="s">image</span><span class="err">:</span> <span class="s">postgres/latest</span>
            <span class="s">ports</span><span class="err">:</span>
            <span class="pi">-</span> <span class="na">containerPort</span><span class="pi">:</span> <span class="m">5432</span>
        <span class="na">volumeMounts</span><span class="pi">:</span> <span class="c1"># connect pod volumes to container</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">postgres-storage</span>
                <span class="s">mountPath</span><span class="err">:</span> <span class="s">/var/lib/postgresql/data</span> <span class="c1"># container dir</span>
                <span class="na">subPath</span><span class="pi">:</span> <span class="s">postgres</span> <span class="c1"># dir created inside vol for container</span>
            <span class="na">env</span><span class="pi">:</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">POSTGRES_PASSWORD</span> <span class="c1"># used to connect to db via API</span>
                <span class="na">value</span><span class="pi">:</span> <span class="s">63eaQB9wtLqmNBpg</span>
            <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">POSTGRES_DB</span>
                <span class="s">value</span><span class="err">:</span> <span class="s">appdb</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>database-persistent-volume.yaml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Not actually used, just an example</span>

<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolume</span>
<span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">database-persistent-volume</span>
<span class="na">spec</span><span class="pi">:</span>
    <span class="na">storageClassName</span><span class="pi">:</span> <span class="s">manual</span>
    <span class="na">capacity</span><span class="pi">:</span>
    <span class="na">storage</span><span class="pi">:</span> <span class="s">5Gi</span>
    <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span> <span class="c1"># mounted as r/w by one single node</span>
    <span class="c1"># - ReadOnlyMany # many nodes can mount it as read only</span>
    <span class="na">hostPath</span><span class="pi">:</span> <span class="c1"># development-specific, look into carefully on creation</span>
    <span class="na">path</span><span class="pi">:</span> <span class="s2">"</span><span class="s">/mnt/data"</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>database-persistent-volume-claim.yaml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">PersistentVolumeClaim</span>
<span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">database-persistent-volume-claim</span>
<span class="na">spec</span><span class="pi">:</span>
    <span class="c1">#storageClassName: manual # removing this dynamically creates volume</span>
    <span class="na">accessModes</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">ReadWriteOnce</span>
    <span class="na">resources</span><span class="pi">:</span>
    <span class="na">requests</span><span class="pi">:</span>
        <span class="na">storage</span><span class="pi">:</span> <span class="s">2Gi</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>postgres-cluster-ip-service.yaml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">postgres-cluster-ip-service</span>
<span class="na">spec</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">ClusterIP</span>
    <span class="na">selector</span><span class="pi">:</span>
    <span class="na">component</span><span class="pi">:</span> <span class="s">postgres</span>
    <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">5432</span>
        <span class="na">targetPort</span><span class="pi">:</span> <span class="m">5432</span>
</code></pre></div>        </div>
      </li>
      <li>
        <p>app-lb-service.yaml</p>

        <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
    <span class="na">name</span><span class="pi">:</span> <span class="s">app-lb-service</span>
<span class="na">spec</span><span class="pi">:</span>
    <span class="na">type</span><span class="pi">:</span> <span class="s">LoadBalancer</span>
    <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">3000</span>
        <span class="na">targetPort</span><span class="pi">:</span> <span class="m">3000</span>
    <span class="na">selector</span><span class="pi">:</span>
    <span class="na">component</span><span class="pi">:</span> <span class="s">app</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
  <li>
    <p>docker-compose.yaml</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">version</span><span class="pi">:</span> <span class="s2">"</span><span class="s">3.8"</span>
  
<span class="na">services</span><span class="pi">:</span> 
    <span class="na">db</span><span class="pi">:</span>
        <span class="na">build</span><span class="pi">:</span>
            <span class="c1"># custom image w/ app table pre-created</span>
            <span class="na">context</span><span class="pi">:</span> <span class="s">./postgres</span>
            <span class="na">dockerfile</span><span class="pi">:</span> <span class="s">Dockerfile.dev</span>
        <span class="na">volumes</span><span class="pi">:</span> 
            <span class="pi">-</span> <span class="s">db-data:/var/lib/postgresql/data</span>
        <span class="na">environment</span><span class="pi">:</span>
            <span class="na">POSTGRES_PASSWORD</span><span class="pi">:</span> <span class="s">63eaQB9wtLqmNBpg</span>
            <span class="na">POSTGRES_DB</span><span class="pi">:</span> <span class="s">notesdb</span>
    <span class="na">app</span><span class="pi">:</span>
        <span class="na">build</span><span class="pi">:</span> 
            <span class="na">context</span><span class="pi">:</span> <span class="s">./app</span>
            <span class="na">dockerfile</span><span class="pi">:</span> <span class="s">Dockerfile.dev</span>
        <span class="na">ports</span><span class="pi">:</span> 
            <span class="pi">-</span> <span class="s">3000:3000</span>
        <span class="na">volumes</span><span class="pi">:</span> 
            <span class="pi">-</span> <span class="s">/home/node/app/node_modules</span>
            <span class="pi">-</span> <span class="s">./app:/home/node/app</span>
        <span class="na">environment</span><span class="pi">:</span> 
            <span class="na">DB_CONNECTION</span><span class="pi">:</span> <span class="s">pg</span>
            <span class="na">DB_HOST</span><span class="pi">:</span> <span class="s">db</span>
            <span class="na">DB_PORT</span><span class="pi">:</span> <span class="m">5432</span>
            <span class="na">DB_USER</span><span class="pi">:</span> <span class="s">postgres</span>
            <span class="na">DB_DATABASE</span><span class="pi">:</span> <span class="s">notesdb</span>
            <span class="na">DB_PASSWORD</span><span class="pi">:</span> <span class="s">63eaQB9wtLqmNBpg</span>
  
<span class="na">volumes</span><span class="pi">:</span>
    <span class="na">db-data</span><span class="pi">:</span>
        <span class="na">name</span><span class="pi">:</span> <span class="s">app-db-dev-data</span>
</code></pre></div>    </div>
  </li>
</ul>

<h3 id="execution-1">Execution</h3>

<ul>
  <li>
    <p>Debug session example:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> app-deployment.yaml
  
<span class="c"># deployment.apps/app-deployment created</span>
  
kubectl get deployment
  
<span class="c"># NAME             READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c"># app-deployment   0/3     3            0           18m2s</span>
  
kubectl get <span class="nt">-f</span> app-deployment.yaml
  
<span class="c"># NAME             READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES               SELECTOR</span>
<span class="c"># app-deployment   0/3     3            0           19m   app          app/latest           component=app</span>
  
kubectl get pod
  
<span class="c"># NAME                             READY   STATUS             RESTARTS   AGE</span>
<span class="c"># app-deployment-d59f9c884-88j45   0/1     CrashLoopBackOff   10         30m</span>
<span class="c"># app-deployment-d59f9c884-96hfr   0/1     CrashLoopBackOff   10         30m</span>
<span class="c"># app-deployment-d59f9c884-pzdxg   0/1     CrashLoopBackOff   10         30m</span>
  
kubectl describe pod api-deployment-d59f9c884-88j45
  
<span class="c"># (Extensive debug info)</span>
  
kubectl logs api-deployment-d59f9c884-88j45
  
<span class="c"># (Logs for container -- error with image)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>After fixing image:</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>kubectl apply <span class="nt">-f</span> api-deployment.yaml
  
<span class="c"># deployment.apps/api-deployment configured</span>
  
kubectl get deployment
  
<span class="c"># NAME             READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c"># app-deployment   3/3     3            3           6m</span>
  
kubectl get pod
  
<span class="c"># NAME                              READY   STATUS    RESTARTS   AGE</span>
<span class="c"># app-deployment-66cdd98546-l9x8q   1/1     Running   0          7m26s</span>
<span class="c"># app-deployment-66cdd98546-mbfw9   1/1     Running   0          7m31s</span>
<span class="c"># app-deployment-66cdd98546-pntxv   1/1     Running   0          7m21s</span>
  
kubectl apply <span class="nt">-f</span> postgres-deployment.yaml
  
<span class="c"># deployment.apps/postgres-deployment created</span>
  
kubectl get deployment
  
<span class="c"># NAME                  READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c"># postgres-deployment   1/1     1            1           13m</span>
  
kubectl get pod
  
<span class="c"># NAME                                   READY   STATUS    RESTARTS   AGE</span>
<span class="c"># postgres-deployment-76fcc75998-mwnb7   1/1     Running   0          13m</span>
  
kubectl apply <span class="nt">-f</span> database-persistent-volume-claim.yaml
  
<span class="c"># persistentvolumeclaim/database-persistent-volume-claim created</span>
  
kubectl get persistentvolumeclaim
  
<span class="c"># NAME                               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span>
<span class="c"># database-persistent-volume-claim   Bound    pvc-525ae8af-00d3-4cc7-ae47-866aa13dffd5   2Gi        RWO            standard       2s</span>
  
kubectl apply <span class="nt">-f</span> postgres-cluster-ip-service.yaml
kubectl apply <span class="nt">-f</span> app-lb-service.yaml
</code></pre></div>    </div>
  </li>
  <li>
    <p>Short version:</p>

    <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">kubectl apply -f k8s</span>
  
<span class="c1"># deployment.apps/app-deployment created</span>
<span class="c1"># service/app-load-balancer-service created</span>
<span class="c1"># persistentvolumeclaim/database-persistent-volume-claim created</span>
<span class="c1"># service/postgres-cluster-ip-service created</span>
<span class="c1"># deployment.apps/postgres-deployment created</span>
  
<span class="s">kubectl get deployment</span>
  
<span class="c1"># NAME                  READY   UP-TO-DATE   AVAILABLE   AGE</span>
<span class="c1"># app-deployment        3/3     3            3           106s</span>
<span class="c1"># postgres-deployment   1/1     1            1           106s</span>
</code></pre></div>    </div>
  </li>
</ul>

<h2 id="further-education">Further Education</h2>

<ul>
  <li><a href="https://kubernetes.io/docs/home/">Kubernetes Documentation</a>: go through ‚ÄúGetting Started‚Äù, ‚ÄúTutorials‚Äù, ‚ÄúConcepts‚Äù.</li>
  <li><a href="https://kubernetes.io/docs/tasks/">Task Catalog</a>: How to do common Kubernetes tasks.</li>
  <li><a href="https://kubernetes.io/docs/reference/">Reference page</a> - all things API, setup (kubeadm) and component (kubelet, kube-proxy, TLS bootstrapping, etc) tools, kubectl, etc.</li>
</ul>]]></content><author><name>Lucas</name></author><category term="kubernetes" /><category term="devops" /><category term="kubernetes" /><category term="containers" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Terraform + Terragrunt Basics</title><link href="https://lucas-sousa.com/terraform-terragrunt-basics/" rel="alternate" type="text/html" title="Terraform + Terragrunt Basics" /><published>2021-12-11T00:00:00-08:00</published><updated>2021-12-11T00:00:00-08:00</updated><id>https://lucas-sousa.com/terraform-terragrunt-basics</id><content type="html" xml:base="https://lucas-sousa.com/terraform-terragrunt-basics/"><![CDATA[<!--üî¥ üü† ‚ö´ ‚ö™ üü£ üü¢ üü° üîµ-->

<h2 id="basics">Basics</h2>

<h3 id="commands">Commands</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">terraform init</code> creates the Remote State: configures your local settings, downloads provider requirements, similar to activating a Python virtualenv</li>
  <li><code class="language-plaintext highlighter-rouge">terraform plan</code> compares current state with desired state</li>
  <li><code class="language-plaintext highlighter-rouge">terraform apply</code> makes the necessary API calls to create/delete/update the resources as needed, then polls them periodically until it is in its desired state</li>
  <li><code class="language-plaintext highlighter-rouge">terraform graph</code> creates a dependency graph with all resources</li>
  <li><code class="language-plaintext highlighter-rouge">terraform destroy</code> removes everything in the state file</li>
</ul>

<h3 id="variables">Variables</h3>

<p><img src="/assets/images/2021-12-11-terraform-terragrunt-basics/tf-variables.png" alt="Terraform Variables" /></p>

<ul>
  <li>since there is no value declared, if you run <code class="language-plaintext highlighter-rouge">terraform plan</code>, it will prompt for input</li>
  <li>you can declare a value via CLI with <code class="language-plaintext highlighter-rouge">terraform apply -var name=foo</code>, or inside terraform.tfvars: <code class="language-plaintext highlighter-rouge">name = "foo"</code></li>
</ul>

<h3 id="dependencies">Dependencies</h3>

<ul>
  <li>reference a resource by their resource type and their name, eg: <code class="language-plaintext highlighter-rouge">aws_instance.example1</code></li>
</ul>

<p><img src="/assets/images/2021-12-11-terraform-terragrunt-basics/tf-dependencies.png" alt="Terraform Dependencies" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">aws_eip</code> requires an instance‚Äôs ID to be specified</li>
  <li>ID is an attribute of <code class="language-plaintext highlighter-rouge">aws_instance</code> that Terraform discovers, sets, and exports on creation</li>
  <li>terraform creates a dependency graph under the hood to figure out the order to create each resource based on dependencies, which can be viewed with <code class="language-plaintext highlighter-rouge">terraform graph</code></li>
</ul>

<h3 id="state">State</h3>

<ul>
  <li>by default, state is stored locally in <code class="language-plaintext highlighter-rouge">.tfstate</code> files, but state can be stored remotely (for collaboration) using S3</li>
</ul>

<p><img src="/assets/images/2021-12-11-terraform-terragrunt-basics/tf-backend.png" alt="Terraform Backend" /></p>

<p><img src="/assets/images/2021-12-11-terraform-terragrunt-basics/tf-backend-lock.png" alt="Terraform Backend with Lock" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">terraform init</code> configures and connects to remote state storage</li>
  <li>most remote backends support locking and encryption</li>
  <li>locking: while using Terraform, you can request a lock and no one will be able to change the state</li>
  <li>lock is stored in DynamoDB</li>
  <li>encryption: terraform stores secrets in plaintext inside state file</li>
</ul>

<h2 id="modules">Modules</h2>

<ul>
  <li>collection of Terraform code that you can reuse, configure, and version control ‚Äî like blueprints</li>
  <li>module = folder with Terraform files</li>
  <li>convention for Gruntwork modules:
    <ol>
      <li>
        <p><code class="language-plaintext highlighter-rouge">vars.tf</code> - specify module inputs</p>

        <p><img src="/assets/images/2021-12-11-terraform-terragrunt-basics/tf-vars.png" alt="vars.tf Example" /></p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">main.tf</code> - create resources</p>

        <p><img src="/assets/images/2021-12-11-terraform-terragrunt-basics/tf-main.png" alt="main.tf Example" /></p>
      </li>
      <li>
        <p><code class="language-plaintext highlighter-rouge">outputs.tf</code> - specify outputs (can be used as input for other modules)</p>

        <p><img src="/assets/images/2021-12-11-terraform-terragrunt-basics/tf-outputs.png" alt="outputs.tf Example" /></p>
      </li>
    </ol>
  </li>
</ul>

<p><img src="/assets/images/2021-12-11-terraform-terragrunt-basics/tf-module-code.png" alt="Terraform Module Code" /></p>

<ul>
  <li>a source can be a directory or URL</li>
  <li>reference module outputs as attributes of that module</li>
  <li>the <code class="language-plaintext highlighter-rouge">module</code> block needs to specify values for variables that don‚Äôt have default value ‚Äî it cannot be passed via CLI and will not be prompted during plan/apply</li>
</ul>

<h3 id="example">Example</h3>

<p><img src="/assets/images/2021-12-11-terraform-terragrunt-basics/tf-module-directory.png" alt="Terraform Module Directory" /></p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">./main.tf</code> - only three block types: terraform (version), provider ‚Äúaws‚Äù, and module blocks w/ the module‚Äôs variables specified inside the block</li>
  <li><code class="language-plaintext highlighter-rouge">./vars.tf</code> - declaration (no values) of variables used inside <code class="language-plaintext highlighter-rouge">main.tf</code>. Since there‚Äôs no <code class="language-plaintext highlighter-rouge">terraform.tfvars</code>, the values will need to be passed via CLI or prompt</li>
  <li><code class="language-plaintext highlighter-rouge">./outputs.tf</code> - outputs values from a module‚Äôs output via <code class="language-plaintext highlighter-rouge">module.module_label.output_label</code></li>
  <li><code class="language-plaintext highlighter-rouge">./rails-module/main.tf</code> - has terraform (version) and resource blocks, no provider block needed</li>
  <li><code class="language-plaintext highlighter-rouge">./rails-module/vars.tf</code> - variables used in <code class="language-plaintext highlighter-rouge">[./rails-module/main.tf](http://main.tf)</code> and declared here need to be addressed inside module block (see <code class="language-plaintext highlighter-rouge">./main.tf</code>)</li>
  <li><code class="language-plaintext highlighter-rouge">./rails-module/outputs.tf</code> - outputs values from <code class="language-plaintext highlighter-rouge">./rails-module/main.tf</code> via <code class="language-plaintext highlighter-rouge">resource_type.resource_label_attribute</code></li>
</ul>

<h3 id="module-community-standards">Module Community Standards</h3>

<ul>
  <li><code class="language-plaintext highlighter-rouge">variables.tf</code> instead of vars.tf</li>
  <li>description property to each variable and output + a README file</li>
</ul>

<h2 id="best-practices">Best Practices</h2>

<ul>
  <li><code class="language-plaintext highlighter-rouge">plan</code> before <code class="language-plaintext highlighter-rouge">apply</code></li>
  <li>stage before prod</li>
  <li>
    <p>isolated environments</p>

    <p><img src="/assets/images/2021-12-11-terraform-terragrunt-basics/tf-module-env.png" alt="Terraform Module Environment" /></p>

    <ul>
      <li>there is a special data source called <code class="language-plaintext highlighter-rouge">terraform_remote_state</code> that shares state between environments ‚Äî you can read outputs from one environment into another</li>
    </ul>
  </li>
  <li>use versioned modules
    <ul>
      <li>
        <p>if modules are defined in a separate repository, different environments can use different versioned URLs</p>

        <p><img src="/assets/images/2021-12-11-terraform-terragrunt-basics/tf-module-version.png" alt="Terraform Module Version" /></p>
      </li>
      <li>
        <p>the <code class="language-plaintext highlighter-rouge">source</code> URL supports versioning using the <code class="language-plaintext highlighter-rouge">ref</code> parameter to point to a tag or a commit ID</p>

        <p><img src="/assets/images/2021-12-11-terraform-terragrunt-basics/tf-module-source.png" alt="Terraform Module Source" /></p>
      </li>
    </ul>
  </li>
  <li>use remote state
    <ul>
      <li>you have to create the bucket yourself ‚Äî enabling versioning is recommended
        <ul>
          <li>the lock table is automatically created</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>Lucas</name></author><category term="terraform" /><category term="devops" /><category term="terraform" /><category term="terragrunt" /><category term="iac" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">AWS Developer Associate Certification Notes</title><link href="https://lucas-sousa.com/aws-developer-associate-notes/" rel="alternate" type="text/html" title="AWS Developer Associate Certification Notes" /><published>2021-12-04T00:00:00-08:00</published><updated>2021-12-04T00:00:00-08:00</updated><id>https://lucas-sousa.com/aws-developer-associate-notes</id><content type="html" xml:base="https://lucas-sousa.com/aws-developer-associate-notes/"><![CDATA[<!--üî¥ üü† ‚ö´ ‚ö™ üü£ üü¢ üü° üîµ-->

<h2 id="api-gateway">API Gateway</h2>

<ul>
  <li>Error codes:
    <ul>
      <li>403: Access denied (authentication or authorization issues)</li>
      <li>429: Limit exceeded or too many requests (throttling)</li>
      <li>502: Incompatible output</li>
      <li>504: INTEGRATION_FAILURE or INTEGRATION_TIMEOUT (default = 29s for all integration types)</li>
    </ul>
  </li>
  <li>Lambda authorizers:
    <ul>
      <li>an API Gateway feature that uses a Lambda function to control access to your API</li>
      <li>takes the caller‚Äôs identity as input and returns an IAM policy as output</li>
      <li>token-based: eceives the caller‚Äôs identity in a bearer token, such as a JSON Web Token (JWT) or an OAuth token</li>
      <li>request parameter-based: eceives the caller‚Äôs identity in a combination of headers, query string parameters, stageVariables, and $context variables</li>
    </ul>
  </li>
  <li>Non-proxy Lambda integration:</li>
</ul>

<p><img src="/assets/images/api_gateway_lambda_integration.png" alt="API Gateway Lambda Integration" /></p>

<h2 id="cloudformation">CloudFormation</h2>

<ul>
  <li>StackSets extends the functionality of stacks by enabling you to create, update, or delete stacks across multiple accounts and regions with a single operation. Using an administrator account, you define and manage an AWS CloudFormation template, and use the template as the basis for provisioning stacks into selected target accounts across specified regions.</li>
  <li>Under the <code class="language-plaintext highlighter-rouge">AWS::Lambda::Function</code> resource, you can use the Code property which contains the deployment package for a Lambda function. For all runtimes, you can specify the location of an object in Amazon S3 (<code class="language-plaintext highlighter-rouge">S3Key</code> and <code class="language-plaintext highlighter-rouge">S3Bucket</code>). For Node.js and Python functions, you can specify the function code inline in the template using <code class="language-plaintext highlighter-rouge">ZipFile</code>.</li>
  <li>The existing Parameters section of a template to define Systems Manager parameters. Systems Manager parameters refer to actual values in the Parameter Store. The value for this type of parameter would be the Systems Manager (SSM) parameter key instead of a string or other value. CloudFormation will fetch values stored against these keys in Systems Manager in your account and use them for the current stack operation.
    <ul>
      <li>As parameters are updated in Systems Manager, you can have the new value of the parameter take effect by just executing a stack update operation.</li>
    </ul>
  </li>
</ul>

<h2 id="cloudwatch">CloudWatch</h2>

<ul>
  <li>Alarms:
    <ul>
      <li>Period: length of time in seconds to evaluate each data point</li>
      <li>Evaluation period: # of most recent periods/data points to evaluate</li>
      <li>Datapoints to alarm: how many breached data points needed in evaluation period to set state to ALARM</li>
    </ul>
  </li>
  <li>CloudWatch does not monitor the memory, swap, and disk space utilization of your instances.</li>
  <li>Monitoring:
    <ul>
      <li>Basic: 5 minutes, N/A for ELB and RDS</li>
      <li>Detailed: 1 minute,  N/A for EBS</li>
      <li>Custom: install CloudWatch agent and configure it to send custom metrics.
        <ul>
          <li>Standard resolution: 1 minute</li>
          <li>High resolution: down to 1 second</li>
        </ul>
      </li>
      <li>Enhanced: Real time metrics for RDS. CloudWatch gathers metrics about CPU utilization from the hypervisor for a DB instance. In contrast, Enhanced Monitoring gathers its metrics from an agent on the DB instance.</li>
    </ul>
  </li>
  <li>RDS logs available via DB parameters:
    <ul>
      <li>Audit log</li>
      <li>Error log (default)</li>
      <li>General log</li>
      <li>Slow query log</li>
    </ul>
  </li>
  <li>Other definitions:
    <ul>
      <li>Dimension: name/value pair that is part of the identity of a metric</li>
      <li>Metric Math: used to query or create time series based on multiple multiple metrics</li>
      <li>Namespace: grouping of metrics aggregated together</li>
    </ul>
  </li>
</ul>

<h2 id="codebuild">CodeBuild</h2>

<ul>
  <li>Fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy</li>
</ul>

<h2 id="codecommit">CodeCommit</h2>

<ul>
  <li>Credentials: must have permissions to access AWS resources (such as CodeCommit repositories) and your IAM user, used to manage your Git credentials or the SSH public key for Git connections. With HTTPS connections and Git credentials, you generate a static user name and password in IAM. You then use these credentials with Git and any third-party tool that supports Git user name and password authentication.</li>
</ul>

<h2 id="codedeploy">CodeDeploy</h2>

<ul>
  <li>AppSpec <code class="language-plaintext highlighter-rouge">hooks</code>:
    <ul>
      <li>EC2/on-prem: mappings that link deployment lifecycle event hooks to one or more scripts</li>
      <li>Lambda/ECS: specifies Lambda validation functions to run during a deployment lifecycle event. An AWS Lambda hook is one Lambda function specified with a string on a new line after the name of the lifecycle event. Each hook is executed once per deployment. Two options: <code class="language-plaintext highlighter-rouge">BeforeAllowTraffic</code> and <code class="language-plaintext highlighter-rouge">AfterAllowTraffic</code></li>
    </ul>
  </li>
  <li>CodeDeploy Agent:
    <ul>
      <li>software package that, when installed and configured on an instance, makes it possible for that instance to be used in CodeDeploy deployments</li>
      <li>communicates outbound using HTTPS over port 443</li>
      <li>only needed for EC2/on prem</li>
    </ul>
  </li>
  <li>Automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services</li>
  <li>Deployment options:
    <ul>
      <li>In-place: app is stopped, new version is installed, started, and validated. Only for EC2/on-prem</li>
      <li>Blue/Green:
        <ul>
          <li>EC2: original environment is replaced by a different set of instances</li>
          <li>Lambda: only option. Traffic shifted between versions. Functions for testing can be specified. Way traffic shift happens can be chosen.</li>
          <li>ECS: Traffic is shifted from the task set with the original version of a containerized application in an ECS service to a replacement task set in the same service. The protocol and port of a specified load balancer listener are used to reroute production traffic. During deployment, a test listener can be used to serve traffic to the replacement task set while validation tests are run.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="cognito">Cognito</h2>

<ul>
  <li>User Pools: just a user directory</li>
  <li>Sync: a client library that enables cross-device syncing of application-related user data</li>
</ul>

<h2 id="dynamodb">DynamoDB</h2>

<ul>
  <li>Expressions:
    <ul>
      <li>Condition Expression: condition for attribute values to select items to be modified</li>
      <li>Filter Expression: filters whole items off of fetched results before returning result</li>
      <li>Projection Expression: fetch only specified attributes from items</li>
    </ul>
  </li>
  <li>Errors:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">ProvisionedThroughputExceededException</code> - more TPUT (WCU or RCU) used than provisioned</li>
      <li><code class="language-plaintext highlighter-rouge">RequestLimitExceeded</code> - provisioned TPUT exceeds account TPUT limit</li>
      <li><code class="language-plaintext highlighter-rouge">ThrottlingException</code> - rate of requests exceeds allowed TPUT</li>
    </ul>
  </li>
  <li>Operations:
    <ul>
      <li>Atomic counter: implemented via <code class="language-plaintext highlighter-rouge">UpdateItem</code>, increments a numeric attribute unconditionally, can be retried, can fail/duplicate</li>
      <li>Conditional write: good for precise operations, no room for error
        <ul>
          <li>Optimistic Locking: record is locked only when changes are committed to the database. Depends on checking a value upon save to ensure that it has not changed.</li>
          <li>Pessimistic Locking: record is locked while it is edited.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Provisioned throughput math:
    <ul>
      <li>One read request unit represents one strongly consistent read request, or two eventually consistent read requests, for an item up to 4 KB in size</li>
      <li>Transactional read requests require 2 read request units to perform one read for items up to 4 KB</li>
    </ul>
  </li>
  <li>Secondary indexes:
    <ul>
      <li>Global: partition and sort key can both be different from base table. On a provisioned table, GSI‚Äôs have their own WCU and RCU. Contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the GSI when you create the index.</li>
      <li>Local: must have same partition key as base table, different sort key. Called local because the index is made within the partition of the base table. Can only be created on table creation. Contains a copy of some or all of the attributes from its base table; you specify which attributes are projected into the LSI when you create the table.</li>
    </ul>
  </li>
  <li>Streams:
    <ul>
      <li>Kinesis Adapter is recommended way to consume streams from DynamoDB for real-time processing. The DynamoDB Streams API is intentionally similar to that of Kinesis Streams</li>
      <li>Lambda isn‚Äôt advised to consume DynamoDB Streams real-time data as it reads records in batches</li>
    </ul>
  </li>
</ul>

<h2 id="ebs">EBS</h2>

<ul>
  <li>New volumes are raw block devices and do not contain any partition or file system.</li>
  <li>Detaching an EBS volume from an EC2 instance:
    <ul>
      <li>terminate instance</li>
      <li>unmount volume or stop instance, then detach
        <ul>
          <li>for root volumes, you must stop instance then detach</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="ec2">EC2</h2>

<ul>
  <li>You can and should use an IAM role to manage temporary credentials for applications that run on an EC2 instance. The role supplies temporary permissions that applications can use when they make calls to other AWS resources. When you launch an EC2 instance, you specify an IAM role to associate with the instance. Applications that run on the instance can then use the role-supplied temporary credentials to sign API requests.
    <ul>
      <li>In order to assign a role to an EC2 instance, you must create an instance profile that is attached to the instance. The instance profile contains the role and can provide the role‚Äôs temporary credentials to an application that runs on the instance.</li>
    </ul>
  </li>
</ul>

<h2 id="ecs">ECS</h2>

<ul>
  <li>Task placement strategy: algorithm for selecting instances for task placement or tasks for termination. Task placement strategies can be specified when either running a task or creating a new service.
    <ul>
      <li>binpack: Place tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use.</li>
      <li>random</li>
      <li>spread: Place tasks evenly based on the specified value. Accepted values are instanceId, host, or a custom attribute key:value pairs such as <code class="language-plaintext highlighter-rouge">attribute:ecs.availability-zone</code>
        <ul>
          <li>Cluster queries are expressions that enable you to group objects. For example, you can group container instances by attributes such as Availability Zone, instance type, or custom metadata. You can add custom metadata to your container instances, known as attributes. Each attribute has a name and an optional string value. You can use the built-in attributes provided by Amazon ECS or define custom attributes.</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="elastic-beanstalk">Elastic Beanstalk</h2>

<ul>
  <li>CLI
    <ul>
      <li>deployment: package app as <code class="language-plaintext highlighter-rouge">zip</code> file and deploy with <code class="language-plaintext highlighter-rouge">eb deploy</code></li>
      <li><code class="language-plaintext highlighter-rouge">aws elasticbeanstalk update-application</code> doesn‚Äôt allow for package upload, only updates specified properties of the app</li>
    </ul>
  </li>
  <li>Deployment methods (note that <code class="language-plaintext highlighter-rouge">canary</code> is not available):</li>
</ul>

<p><img src="/assets/images/elastic_beanstalk_deployment_methods.png" alt="Elastic Beanstalk Deployment Methods" /></p>

<ul>
  <li>Note that Canary is not available</li>
  <li>Environment: an instance of a version of an application. You can have multiple environments of the same version or app at the same time
    <ul>
      <li>Server environment: composed of an ELB and ASG of EC2 instances containing EB app that writes requests to SQS queue</li>
      <li>Worker environment: composed of an ASG of EC2 instances containing EB app and an SQS daemon that pulls requests from EB SQS queue</li>
    </ul>
  </li>
  <li>Files:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">cron.yaml</code> - configuration file primarily used to define periodic tasks that add jobs to your worker environment‚Äôs queue automatically at a regular interval.</li>
      <li><code class="language-plaintext highlighter-rouge">Dockerrun.aws.json</code> - configuration file primarily used in multicontainer Docker environments hosted in Elastic Beanstalk. This can be used alone or combined with source code and content in a source bundle to create an environment on a Docker platform.</li>
      <li><code class="language-plaintext highlighter-rouge">env.yaml</code> - manifest in the root of your application source bundle to configure the environment name, solution stack and environment links to use when creating your environment. The manifest uses the same format as Saved Configurations.</li>
    </ul>
  </li>
  <li>It isn‚Äôt good practice to spin up an RDS instance as part of your EB app as it ties the lifecycle of the database to the lifecycle of app environment. It is better to decouple it by creating an RDS instance separately and configure the app with the necessary information to connect on launch.
    <ul>
      <li>Note that a sec. group cannot be deleted if it is linked to another sec. group by a rule.</li>
    </ul>
  </li>
  <li>Platform Version: New platform versions provide updates to existing software components and support for new features and configuration options. Eg: Java 7 to Java 8
    <ul>
      <li>Two methods to update platform version:
        <ul>
          <li>Update your Environment‚Äôs Platform Version - This is the recommended method when you‚Äôre updating to the latest platform version, without a change in runtime, web server, or application server versions, and without a change in the major platform version. This is the most common and routine platform update.</li>
          <li>Blue/Green Deployment - This is the recommended method when you‚Äôre updating to a different runtime, web server, or application server versions, or to a different major platform version. This is a good approach when you want to take advantage of new runtime capabilities or the latest Elastic Beanstalk functionality.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Usage: create an application, upload an application version in the form of an application source bundle (for example, a Java .war file) to Elastic Beanstalk, and then provide some information about the application. Elastic Beanstalk automatically launches an environment and creates and configures the AWS resources needed to run your code. After your environment is launched, you can then manage your environment and deploy new application versions.</li>
</ul>

<h2 id="elasticache">Elasticache</h2>

<ul>
  <li>Types:
    <ul>
      <li>Redis: rich features. Blazing fast in-memory data store that provides sub-millisecond latency to power internet-scale real-time applications. Redis also lets you create multiple replicas of a Redis primary. This allows you to scale database reads and to have highly available clusters.</li>
      <li>Memcached: simpler. Less available but can provide multiple threads or cores, whereas Redis is mostly a single-threaded server</li>
    </ul>
  </li>
  <li>Strategies:
    <ul>
      <li>Lazy Loading: write to cache on cache miss</li>
      <li>Russian Doll: nested records with top level cache keys (eg. user, stories, comments)</li>
      <li>Write Through: write to cache on write to database</li>
    </ul>
  </li>
</ul>

<h2 id="kinesis-data-streams">Kinesis Data Streams</h2>

<ul>
  <li>Duplication of data: usually caused by producer or consumer retries, can be mitigated by adding a primary key to the record</li>
  <li>KCL worker (EC2 instance) can process any number of shards, but a shard can only have one record processor (KCL worker)</li>
</ul>

<h2 id="kinesis-data-firehose">Kinesis Data Firehose</h2>

<ul>
  <li>The easiest way to load streaming data into data stores and analytics tools.</li>
  <li>It is a fully managed service that automatically scales to match the throughput of your data.</li>
  <li>It can also batch, compress, and encrypt the data before loading it.</li>
</ul>

<h2 id="kinesis-data-analytics">Kinesis Data Analytics</h2>

<ul>
  <li>Analyze streaming data, gain actionable insights, and respond to your business and customer needs in real time. You can quickly build SQL queries and Java applications using built-in templates and operators for common processing functions to organize, transform, aggregate, and analyze data at any scale.</li>
</ul>

<h2 id="kms">KMS</h2>

<ul>
  <li>Envelope encryption is the practice of encrypting plaintext data with a data key and then encrypting the data key with another top-level master key.
    <ul>
      <li>Encrypting data locally:
    1. Use the <code class="language-plaintext highlighter-rouge">GenerateDataKey</code> operation to get a data encryption key.
    2. Use the plaintext data key (returned in the <code class="language-plaintext highlighter-rouge">Plaintext</code> field of the response) to encrypt data locally, then erase the plaintext data key from memory.
    3. Store the encrypted data key (returned in the <code class="language-plaintext highlighter-rouge">CiphertextBlob</code> field of the response) alongside the locally encrypted data.</li>
      <li>Decrypting data locally:
    1. Use the <code class="language-plaintext highlighter-rouge">Decrypt</code> operation to decrypt the encrypted data key. The operation returns a plaintext copy of the data key.
    2. Use the plaintext data key to decrypt data locally, then erase the plaintext data key from memory.</li>
    </ul>
  </li>
</ul>

<h2 id="lambda">Lambda</h2>

<ul>
  <li>CPU: In the AWS Lambda resource model, you choose the amount of memory you want for your function and are allocated proportional CPU power and other resources. An increase in memory size triggers an equivalent increase in CPU available to your function.</li>
  <li>Concurrency for push-based event sources and invocations:
    <ul>
      <li>concurrent executions = (invocations per second) x (average execution duration in seconds)</li>
      <li>default concurrency limit = 1000/region
        <ul>
          <li>minimum unreserved concurrency per region = 100 (ie. you can reserve 900 between your functions, or one function with 900)</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Concurrency for poll-based event source (Kinesis) equals the number of shards.</li>
  <li>Execution context: provides 512 MB of additional disk space in the¬†<code class="language-plaintext highlighter-rouge">/tmp</code> directory. The directory content remains when the execution context is frozen, providing transient cache that can be used for multiple invocations. You can add extra code to check if the cache has the data that you stored.</li>
  <li>Layer: a ZIP archive that contains libraries, a custom runtime, or other dependencies</li>
  <li>InvocationType:
    <ul>
      <li>RequestResponse (Synchronous): default. Connection stays open until function returns something or times out. API response includes the function response</li>
      <li>Event (Asynchronous): events that fail multiple times can be sent to DLQ and API response only includes status code</li>
      <li>Dry-run: validate parameter values and invocation permissions</li>
    </ul>
  </li>
  <li>Runtimes supported: PowerShell, Go, Python, Node.js, Java, C#, Ruby, Custom for everything else</li>
  <li>Traffic Shifting: the <code class="language-plaintext highlighter-rouge">routing-config</code> parameter of a Lambda alias can be implemented for that alias to point to two different versions of a function, as long as they have same IAM exec. role, DLQ config, and none of the versions are $LATEST</li>
</ul>

<h2 id="s3">S3</h2>

<ul>
  <li>CORS config document‚Äôs <code class="language-plaintext highlighter-rouge">&lt;CORSrule&gt;</code> components:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">AllowedOrigin</code> - Specifies domain origins that you allow to make cross-domain requests.</li>
      <li><code class="language-plaintext highlighter-rouge">AllowedMethod</code> - Specifies a type of request you allow (GET, PUT, POST, DELETE, HEAD) in cross-domain requests.</li>
      <li><code class="language-plaintext highlighter-rouge">AllowedHeader</code> - Specifies the headers allowed in a preflight request.</li>
      <li><code class="language-plaintext highlighter-rouge">MaxAgeSeconds</code>  - Specifies the amount of time in seconds (in this example, 3000) that the browser caches an Amazon S3 response to a preflight OPTIONS request for the specified resource. By caching the response, the browser does not have to send preflight requests to Amazon S3 if the original request will be repeated.</li>
      <li><code class="language-plaintext highlighter-rouge">ExposeHeader</code>  - Identifies the response headers (e.g. <code class="language-plaintext highlighter-rouge">x-amz-server-side-encryption</code>, <code class="language-plaintext highlighter-rouge">x-amz-request-id</code>, and <code class="language-plaintext highlighter-rouge">x-amz-id-2</code>) that customers are able to access from their applications (e.g. from a JavaScript <code class="language-plaintext highlighter-rouge">XMLHttpRequest</code> object).</li>
    </ul>
  </li>
  <li>Cross-region replication (CRR) enables automatic, asynchronous copying of objects across buckets in different AWS Regions. Buckets configured for cross-region replication can be owned by the same AWS account or by different accounts. Cross-region replication is enabled with a bucket-level configuration. You add the replication configuration to your source bucket.
    <ul>
      <li>Requirements:
        <ul>
          <li>The source and destination buckets must have versioning enabled.</li>
          <li>The source and destination buckets must be in different AWS Regions.</li>
          <li>Amazon S3 must have permissions to replicate objects from that source bucket to the destination bucket on your behalf.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Encryption:
    <ul>
      <li>SSE-S3 - header: <code class="language-plaintext highlighter-rouge">x-amz-server-side-encryption</code> (example value: <code class="language-plaintext highlighter-rouge">AES256</code>)</li>
      <li>SSE-KMS
        <ul>
          <li>headers:
            <ul>
              <li><code class="language-plaintext highlighter-rouge">x-amz-server-side-encryption</code> (value <code class="language-plaintext highlighter-rouge">aws:kms</code>)</li>
              <li><code class="language-plaintext highlighter-rouge">x-amz-server-side-encryption-aws-kms-key-id</code> (optional, defaults to default CMK for region, which is created automatically by S3)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>SSE-C: customer-provider encryption keys
        <ul>
          <li>headers:
            <ul>
              <li><code class="language-plaintext highlighter-rouge">x-amz-server-side-encryption-customer-algorithm</code></li>
              <li><code class="language-plaintext highlighter-rouge">x-amz-server-side-encryption-customer-key</code></li>
              <li><code class="language-plaintext highlighter-rouge">x-amz-server-side-encryption-customer-key-MD5</code></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Glacier object retrieval:
    <ul>
      <li>Standard: 3-5 hours</li>
      <li>Bulk: 5-12 hours</li>
      <li>Expedited: 1-5 minutes</li>
    </ul>
  </li>
  <li>Multipart Upload: when using a KMS customer master key (CMK), the requester must have permission to the <code class="language-plaintext highlighter-rouge">kms:Decrypt</code> and <code class="language-plaintext highlighter-rouge">kms:GenerateDataKey*</code> actions on the key. These permissions are required because Amazon S3 must decrypt and read data from the encrypted file parts before it completes the multipart upload.</li>
  <li>S3 Transfer Acceleration leverages Amazon CloudFront‚Äôs globally distributed AWS Edge Locations. As data arrives at an AWS Edge Location, data is routed to your Amazon S3 bucket over an optimized network path.</li>
</ul>

<h2 id="sam">SAM</h2>

<ul>
  <li>Consists of the AWS SAM template specification that you use to define your serverless applications, and the AWS SAM command line interface (AWS SAM CLI) that you use to build, test, and deploy your serverless applications. The CLI provides a Lambda-like execution environment locally. It helps you catch issues upfront by providing parity with the actual Lambda execution environment.</li>
</ul>

<h2 id="sqs">SQS</h2>

<ul>
  <li>Delay queue lets you postpone the delivery of new messages to a queue for a number of seconds. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes</li>
  <li>Long polling is a way to retrieve messages from your Amazon SQS queues. While the regular short polling returns immediately, even if the message queue being polled is empty, long polling doesn‚Äôt return a response until a message arrives in the message queue, or the long poll times out.</li>
</ul>

<h2 id="sts">STS</h2>

<ul>
  <li>API calls:
    <ul>
      <li><code class="language-plaintext highlighter-rouge">AssumeRole</code> - needs a role ARN as parameter and returns the temporary credentials to a newly created session with same policies/access as the role.</li>
      <li><code class="language-plaintext highlighter-rouge">AssumeRoleWithSAML</code> - same as above but for users authenticated via SAML, eg. enterprise identity store or directory.</li>
      <li><code class="language-plaintext highlighter-rouge">AssumeRoleWithWebIdentity</code> - same as above but for federated users authenticated via public identity providers such as Google.</li>
      <li><code class="language-plaintext highlighter-rouge">GetFederationToken</code> - doesn‚Äôt support MFA.</li>
      <li><code class="language-plaintext highlighter-rouge">GetSessionToken</code> - returns set of temporary credentials for accounts and users only. The credentials consist of an access key ID, a secret access key, and a security token. Typically used for MFA.</li>
    </ul>
  </li>
</ul>

<h2 id="systems-manager-parameter-store">Systems Manager Parameter Store</h2>

<ul>
  <li>Provides secure, hierarchical storage for configuration data management and secrets management</li>
</ul>

<h2 id="vpc">VPC</h2>

<ul>
  <li>Route tables:
    <ul>
      <li>default limit of 200 route tables per VPC</li>
      <li>you can associate multiple subnets to the same route table</li>
      <li>you cannot modify/edit the main route created by default</li>
      <li>a subnet can only be associated with one route table at a time</li>
    </ul>
  </li>
</ul>

<h2 id="x-ray">X-Ray</h2>

<ul>
  <li>Active Tracing: feature you can enable in Lambda</li>
  <li>Components:
    <ul>
      <li>Annotation: key-value pairs added to traces, segments, and subsegments and are indexed for filter expression searches</li>
      <li>Metadata: same as annotation but not indexed</li>
      <li>Sampling algorithm: default is 1st request of every second + 5% of remaining requests</li>
      <li>Segment: provides the name of compute resource handling request, details about request and work done
        <ul>
          <li>host - host name, alias or IP address</li>
          <li>request - method, client address, path, user agent</li>
          <li>response - status, content</li>
          <li>work done - start and end time, subsegments</li>
          <li>issues - errors, faults, exceptions</li>
        </ul>
      </li>
      <li>Subsegment: additional details about work done
        <ul>
          <li>namespace: field in subsegment. <code class="language-plaintext highlighter-rouge">aws</code> for AWS SDK calls; <code class="language-plaintext highlighter-rouge">remote</code> for other downstream calls</li>
          <li>http: http object with information about an outgoing HTTP call</li>
          <li>aws: aws object with information about the downstream AWS resource that your application called.</li>
          <li>annotation, metadata</li>
          <li>error, throttle, fault, and cause</li>
          <li>precursor_ids: array of subsegment IDs that identifies subsegments with the same parent that completed prior to this subsegment</li>
        </ul>
      </li>
      <li>Trace: string of segments generated by a single request</li>
      <li>Tracing header: added in the HTTP request header (not on the segment document). A tracing header (<code class="language-plaintext highlighter-rouge">X-Amzn-Trace-Id</code>) can originate from the X-Ray SDK, an AWS service, or the client request</li>
    </ul>
  </li>
  <li>ECS:
    <ul>
      <li>create a Docker image that runs the X-Ray daemon</li>
      <li>upload it to a Docker image repository</li>
      <li>deploy it to your Amazon ECS cluster</li>
      <li>use port mappings and network mode settings in your task definition file to allow your application to communicate with the daemon container
        <ul>
          <li>the daemon is a software application that listens for traffic on UDP port 2000, gathers raw segment data, and relays it to the AWS X-Ray API</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Enabling X-Ray:
    <ul>
      <li>Elastic Beanstalk: include xray-daemon.config file in .ebextensions directory of source code or via console</li>
      <li>ECS: create a docker image that runs the X-Ray daemon</li>
    </ul>
  </li>
  <li>Environment variables used by Lambda to facilitate communication with the X-Ray daemon and configure the X-Ray SDK:
    <ul>
      <li>_X_AMZN_TRACE_ID: contains tracing header (sampling decision, tracing ID, parent segment ID)</li>
      <li>AWS_XRAY_CONTEXT_MISSING: used by SDK to determine what to do if no tracing header. Set by Lambda to LOG_ERROR by default</li>
      <li>AWS_XRAY_DAEMON_ADDRESS: formatted IP_ADDRESS:PORT and used to send trace data to daemon directly, without SDK</li>
    </ul>
  </li>
  <li>If a load balancer or other intermediary forwards a request to your application, X-Ray takes the client IP from the <code class="language-plaintext highlighter-rouge">X-Forwarded-For</code> header in the request instead of from the source IP in the IP packet, which would only show the ELB‚Äôs IP address.</li>
  <li>SDK: does not send trace data directly to AWS X-Ray. To avoid calling the service every time your application serves a request, the SDK sends the trace data to a daemon, which collects segments for multiple requests and uploads them in batches.</li>
</ul>]]></content><author><name>Lucas</name></author><category term="aws" /><category term="aws" /><category term="cloud" /><category term="certificate" /><category term="aws developer associate" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Hosting a Safe Static Website on AWS (UI Version)</title><link href="https://lucas-sousa.com/hosting-safe-static-website-aws-ui/" rel="alternate" type="text/html" title="Hosting a Safe Static Website on AWS (UI Version)" /><published>2021-11-27T00:00:00-08:00</published><updated>2021-11-27T00:00:00-08:00</updated><id>https://lucas-sousa.com/hosting-safe-static-website-aws-ui</id><content type="html" xml:base="https://lucas-sousa.com/hosting-safe-static-website-aws-ui/"><![CDATA[<!--üî¥ üü† ‚ö´ ‚ö™ üü£ üü¢ üü° üîµ-->

<p>There are many articles on how to host a static website using S3. Many more explain how to enable <em>https</em>, and plenty of them lay out how to use your own domain to host them. AWS‚Äô documentation alone covers all of it. In fact, the base for this article is in the documentation <a href="https://docs.aws.amazon.com/AmazonS3/latest/userguide/website-hosting-custom-domain-walkthrough.html">pages</a>.</p>

<p>What makes this article worth writing (and hopefully reading) is the attention to the rationale behind every step, from bucket configuration to A record creation. The idea is to learn concepts rather than to practice following steps. Details on what buttons to click and where to find them can be found on the documentation.</p>

<p><strong>Disclaimer</strong>: some steps can or will incur costs. Many of the steps are optional in hosting a website, such as logging, having a custom domain, or using a CDN.</p>

<h2 id="01-register-custom-domain">01. Register custom domain</h2>

<p>Route 53 offers both domain registration and DNS hosting services. Domain registration means you pay (usually a yearly fee) for the domain name, eg. <em>domain-name.com</em>. AWS will use a partner registrar to register your information as the owner of that domain. The other thing it will do is keep track of the authoritative name servers for that domain, ie. the servers known to be in charge of knowing what IP to translate your domain name to.</p>

<p>That is where their DNS hosting service comes in. For a monthly fee, Route 53 provides you with servers that you can select to be your domain‚Äôs authoritative name servers. Usually those will be automatically generated by Route 53 and configured into your domain registration. They‚Äôre called <em>Hosted Zones</em> and come in sets of four servers.</p>

<h2 id="02-create-and-validate-certificate">02. Create and validate certificate</h2>

<p>You can do this step with the AWS Certificate Manager (ACM) service. When you create your certificate, it is a good idea to add both your apex domain name and a wildcard for all subdomains, eg. <em>mydomain.com</em> and <em>*.mydomain.com</em>, so that all your pages are covered by the certificate. The certificate is nothing more than proof that the person who owns the domain is providing the content. In order to earn the requested certificate from the certificate authority (CA), you need to prove that you control the domain in question.</p>

<p>The way to do that with ACM is easy: they provide you a subdomain name (eg. <em>_e7a8d1a9c01ec67ee57e0941f2b43a39</em>) and an AWS page that the subdomain should point to (eg. <em>_e87d0bf50c515881ed3a703a6216e409.wrnxprcrrv.acm-validations.aws.</em>) for each domain name in the certificate, which usually are the apex and the wildcard names mentioned above. All that is needed is to create the equivalent CNAME records. ACM has a button that automatically does the record creation. Within seconds, your certificate should be issued and ready to be used.</p>

<h2 id="03-create-and-configure-hosting-bucket">03. Create and configure hosting bucket</h2>

<p>It is strongly encouraged that the hosting bucket have the same name as the domain, specially if you‚Äôre not using CloudFront. The reason for that is that S3 buckets don‚Äôt have unique IP addresses. Their IP isn‚Äôt even static. It just uses an IP from a vast pool of AWS IP addresses. Therefore, when a request is made to a bucket, it arrives to one of the many public AWS IP addresses, and a <code class="language-plaintext highlighter-rouge">host</code> header (the domain name in the URL) is extracted, and AWS uses that value to locate the bucket. There is no extra logic behind that. So if AWS fields a request to a bucket website addressed to <em>jenkins.mydomain.com/status</em>, it will look for bucket <em>jenkins.mydomain.com</em>. If the request is for <em>my-example.s3-website.us-east-1.amazonaws.com/</em>, it will look for bucket <em>my-example</em>.</p>

<p>On the other hand, using CloudFront would work because it rewrites the <code class="language-plaintext highlighter-rouge">host</code> header. The CloudFront distribution has an <em>origins</em> setting where the bucket, ELB, or website serving the content can be listed. Another workaround to mismatching bucket and domain names is to set up a same-region EC2 reverse proxy. One workaround that does not work is to create a CNAME record from <em>mydomain.com</em> to <em>my-example.s3-website.us-east-1.amazonaws.com/</em>. The <em>mydomain.com</em> header is kept in the request through redirections, so when the request arrives to AWS for the server responsible for <em>my-example.s3-website.us-east-1.amazonaws.com/</em>, it won‚Äôt serve anything because it doesn‚Äôt have anything to serve for requests to <em>mydomain.com</em>.</p>

<p><strong>Important</strong>: when using CloudFront, website users contact the CDN and the CDN contacts the bucket for the files, so the bucket should block all public access. CloudFront has a feature called <em>origin access identities</em> that can be created and referred to in the bucket policy to give the service read access. If CloudFront is not being used, then public access needs to be allowed in the bucket permissions.</p>

<h2 id="04-create-and-configure-logging-bucket">04. Create and configure logging bucket</h2>

<p>The bucket should be in the same region as the hosting bucket for cost reasons. The main detail for this step is the permission configuration. By default, only the account owner can write to the bucket, so object-level ACLs are disabled. CloudFront can add a bucket policy entry to give itself permission to write logs, but you must manually enable ACLs for it to work.</p>

<h2 id="05-add-content-to-hosting-bucket">05. Add content to hosting bucket</h2>

<p>This part is highly depends on personal preferences and objectives. You can add a personal project using CSS and Javascript, you can add a whole <a href="https://jekyllrb.com/">Jekyll</a> blog, or you can simply add an index.html page for testing.</p>

<p>If you‚Äôre going the Jekyll route, or any other route where you manipulate your links, be very wary. In my case, changing the <code class="language-plaintext highlighter-rouge">permalink</code> of pages caused all requested pages other than the root page to not be found. But the worst part was that I did not get a <em>Page Not Found</em> error. Instead, I got an <em>Access Denied</em> error, making it much harder to find the problem. To summarize, I had to create a CloudFront Function that added <code class="language-plaintext highlighter-rouge">index.html</code> to all requests that ended with a <code class="language-plaintext highlighter-rouge">/</code>. You can read more about that <a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/example-function-add-index.html">here</a>.</p>

<h2 id="06-create-and-configure-cdn-distribution">06. Create and configure CDN distribution</h2>

<p>By default, CloudFront only knows how to serve requests to its domain name, which should be something like <em><code class="language-plaintext highlighter-rouge">https://xxxxxxxxxxxxxx.cloudfront.net</code></em>. In order to let it know it is okay to serve requests that are originally sent to your custom domain (<em>mydomain.com</em>). Step 3 dives a bit deeper on this topic. Step 7 will not work if your alternate domain names are not properly set.</p>

<p>This step is where the certificate created in step 2 comes into play as well. You can cover your alternate domain names with your SSL certificate so that you can redirect all requests to HTTPS.</p>

<p>A quick note on origin domain: it is best practice to use the domain name version that includes the region of your bucket. The global domain of the bucket works for most regions, but not all.</p>

<h2 id="07-create-alias-records">07. Create alias records</h2>

<p>If you followed the previous steps correctly, this step should be simple. Go to the hosted zone of your custom domain and create an alias A record to your CloudFront distribution. If your CloudFront distribution doesn‚Äôt show up as an option when you create the record via Route 53, chances are that you did not properly configure your distribution‚Äôs alternate domain names to use your custom domain name. Due to reasons touched upon on steps 3 and 6, Route 53 is configured to not allow the creation of aliases to CloudFront distributions that do not have the domain (or subdomain) as an alternate domain name.</p>]]></content><author><name>Lucas</name></author><category term="networking" /><category term="aws" /><category term="aws" /><category term="cloud" /><category term="route 53" /><category term="cloudfront" /><category term="acm" /><category term="s3" /><category term="webhosting" /><category term="dns" /><summary type="html"><![CDATA[]]></summary></entry></feed>